{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - More routing iterations: Does more routing help classification. (This routing operation is sweet, can we benefit from it more)\n",
    " \n",
    " - Varying the number of capsules: Does the number(32)/size(8) of the PrimaryCaps layer help classification? (This network is heavy as f!@#)\n",
    " \n",
    " - More initial convolutional layers: Do smaller filters (3x3, say) with more standard conv layers help classification? (This network is heavy as f!@#)\n",
    " \n",
    " - No primary capsules: Can we get away with just convolutional layers, routing, DigiCaps and reconstruction?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(inputs, filters, kernel_size=9, strides=1, padding='valid', activation=None):\n",
    "    return tf.layers.conv2d(inputs=inputs, \n",
    "                            filters=filters, \n",
    "                            kernel_size=kernel_size, \n",
    "                            strides=strides, \n",
    "                            padding=padding, \n",
    "                            activation=activation, \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "ROUTING_ITERS = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6, 2) (2, 6, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(2*3*2).reshape((2, 3, 2))\n",
    "y = np.arange(2*3*2).reshape((2, 3, 2)) + 50\n",
    "M = np.ones(6*3*2).reshape((6, 3, 2))\n",
    "M = np.tile(M, (2, 1, 1, 1))\n",
    "b = np.stack([x, y], axis=1).reshape((2, 3*2, 2))\n",
    "print(b.shape, M.shape)\n",
    "# print(b)\n",
    "# np.einsum('ijk,nmpk->ijnmp', b, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 43  46]\n",
      "  [127 134]\n",
      "  [243 254]]\n",
      "\n",
      " [[211 226]\n",
      "  [343 362]\n",
      "  [507 530]]]\n"
     ]
    }
   ],
   "source": [
    "n_pc = 4\n",
    "pc = 2\n",
    "dc = 3\n",
    "a = tf.constant(np.arange(1, 1 + n_pc*pc, dtype=np.int32),\n",
    "                shape=[2, n_pc, pc])\n",
    "b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
    "                shape=[1, n_pc, pc, dc])\n",
    "b = tf.tile(b, (2, 1, 1, 1))\n",
    "a = tf.reshape(a, shape=(2, 3, 1, 2)) \n",
    "c = tf.matmul(a, b)\n",
    "c = tf.reshape(c, shape=(2, 3, 2))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6875.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caps_net_model_fn(features, labels, mode):\n",
    "    \n",
    "    # image layer\n",
    "    input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\n",
    "    \n",
    "    # save the batch size for later (<= 64)\n",
    "    batch_size = input_layer.get_shape()[0]\n",
    "    n_primary_caps = 32\n",
    "    primary_caps_size = 8\n",
    "    digi_caps_size = 16\n",
    "    \n",
    "    # conv layer (regular convolutional layer)\n",
    "    with tf.variable_scope('conv_layer'):\n",
    "        conv1 = conv_layer(input_layer, 256, activation=tf.nn.relu)\n",
    "        assert conv1.get_shape() == [batch_size, 20, 20, 256]\n",
    "    \n",
    "    # first capsule layer (PrimaryCaps)\n",
    "    capsules = []\n",
    "    for i in range(32):\n",
    "        with tf.variable_scope('capsule_1_' + str(i + 1)):\n",
    "            caps_i = conv_layer(conv1, primary_caps_size, strides=2)\n",
    "            reshape = tf.reshape(caps_i, shape=(-1, 6*6, primary_caps_size))\n",
    "            capsules.append(reshape)\n",
    "    assert capsules[0].get_shape() == [batch_size, 6*6, primary_caps_size]\n",
    "    \n",
    "    # stack and reshape\n",
    "    #\n",
    "    # here we reshape the capsule outputs (i.e. the 8D vectors)\n",
    "    # to be 1x8 matrices, this will enable us to use tf.matmul\n",
    "    # to calculate the inputs (u_hat_ij's) to each DigiCaps\n",
    "    # capsule in one shot\n",
    "    capsules = tf.stack(capsules, axis=1)\n",
    "    capsules = tf.reshape(capsules, shape=(-1, 6*6*n_primary_caps, 1, primary_caps_size))\n",
    "    assert capsules.get_shape() == [batch_size, 6*6*n_primary_caps, 1, primary_caps_size]\n",
    "    \n",
    "    # second capsule layer (DigiCaps)\n",
    "    u_hat = []\n",
    "    for j in range(10):\n",
    "        with tf.variable_scope('->capsule_{}'.format(j)):\n",
    "            name = 'W_i{}'.format(j)\n",
    "            weights_to_j = tf.get_variable(name=name, shape=(1, 6*6*n_primary_caps, primary_caps_size, digi_caps_size))\n",
    "            weights_to_j = tf.tile(weights_to_j, (batch_size, 1, 1, 1))\n",
    "            u_hat_ji = tf.matmul(capsules, weights_to_j, )\n",
    "            \n",
    "    \n",
    "    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
